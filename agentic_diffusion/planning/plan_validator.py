"""
Plan validation for AdaptDiffuser planning.

This module contains classes and functions for validating plans and trajectories
generated by AdaptDiffuser, ensuring they meet task-specific constraints.
"""

import torch
import numpy as np
from typing import Dict, List, Optional, Union, Any, Tuple, Callable
import logging
from abc import ABC, abstractmethod

# Configure logging
logger = logging.getLogger(__name__)


class PlanValidator(ABC):
    """
    Base class for plan validators in AdaptDiffuser planning.
    
    Plan validators check whether generated plans and trajectories
    satisfy task-specific constraints and requirements.
    """
    
    def __init__(
        self,
        device: str = None
    ):
        """
        Initialize the plan validator.
        
        Args:
            device: Device to use for computation
        """
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    
    @abstractmethod
    def validate(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        task: Optional[Any] = None
    ) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """
        Validate a plan or trajectory.
        
        Args:
            states: Tensor of states in the trajectory
            actions: Tensor of actions in the trajectory
            task: Optional task specification
            
        Returns:
            Tuple of (validity mask, detailed constraint information)
        """
        pass
    
    @abstractmethod
    def compute_violation_penalties(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        task: Optional[Any] = None
    ) -> torch.Tensor:
        """
        Compute violation penalties for constraint-aware planning.
        
        Args:
            states: Tensor of states in the trajectory
            actions: Tensor of actions in the trajectory
            task: Optional task specification
            
        Returns:
            Tensor of violation penalties for each trajectory
        """
        pass


class RuleBasedValidator(PlanValidator):
    """
    Rule-based validator that checks explicit constraint rules.
    
    This validator applies a set of user-defined rules to validate plans.
    """
    
    def __init__(
        self,
        constraint_functions: List[Callable],
        constraint_weights: Optional[List[float]] = None,
        device: str = None
    ):
        """
        Initialize the rule-based validator.
        
        Args:
            constraint_functions: List of constraint checking functions
            constraint_weights: Optional weights for each constraint
            device: Device to use for computation
        """
        super().__init__(device)
        self.constraint_functions = constraint_functions
        
        # Set uniform weights if not provided
        if constraint_weights is None:
            constraint_weights = [1.0] * len(constraint_functions)
        elif len(constraint_weights) != len(constraint_functions):
            raise ValueError("Number of weights must match number of constraint functions")
            
        self.constraint_weights = torch.tensor(
            constraint_weights, dtype=torch.float32, device=self.device
        )
    
    def validate(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        task: Optional[Any] = None
    ) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """
        Validate plans against all constraint rules.
        
        Args:
            states: Tensor of states in the trajectory, shape [batch_size, seq_len, state_dim]
            actions: Tensor of actions in the trajectory, shape [batch_size, seq_len, action_dim]
            task: Optional task specification
            
        Returns:
            Tuple of (validity mask of shape [batch_size], detailed constraint information)
        """
        batch_size = states.shape[0]
        num_constraints = len(self.constraint_functions)
        
        # Initialize constraint violations tensor
        # For each trajectory, track whether each constraint is violated
        violations = torch.zeros(
            (batch_size, num_constraints), dtype=torch.bool, device=self.device
        )
        
        # Check each constraint
        for i, constraint_fn in enumerate(self.constraint_functions):
            # Constraint functions return True for violations
            violations[:, i] = constraint_fn(states, actions, task)
        
        # A plan is valid if it violates no constraints
        validity = ~torch.any(violations, dim=1)
        
        # Compute detailed constraint information
        constraint_info = {
            "violations": violations,
            "violation_counts": torch.sum(violations, dim=1),
            "violation_rates": {
                f"constraint_{i}": violations[:, i].float().mean().item()
                for i in range(num_constraints)
            }
        }
        
        return validity, constraint_info
    
    def compute_violation_penalties(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        task: Optional[Any] = None
    ) -> torch.Tensor:
        """
        Compute weighted violation penalties.
        
        Args:
            states: Tensor of states in the trajectory
            actions: Tensor of actions in the trajectory
            task: Optional task specification
            
        Returns:
            Tensor of violation penalties for each trajectory
        """
        batch_size = states.shape[0]
        num_constraints = len(self.constraint_functions)
        
        # Initialize violations tensor
        violations = torch.zeros(
            (batch_size, num_constraints), dtype=torch.float32, device=self.device
        )
        
        # Check each constraint
        for i, constraint_fn in enumerate(self.constraint_functions):
            # Convert boolean violations to float
            violations[:, i] = constraint_fn(states, actions, task).float()
        
        # Apply weights to violations
        weighted_violations = violations * self.constraint_weights
        
        # Sum violations for each trajectory
        penalties = torch.sum(weighted_violations, dim=1)
        
        return penalties


class LearningBasedValidator(PlanValidator):
    """
    Learning-based validator that uses neural networks for constraint checking.
    
    This validator learns to identify valid plans from examples.
    """
    
    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        hidden_dims: List[int] = [128, 64, 32],
        device: str = None
    ):
        """
        Initialize the learning-based validator.
        
        Args:
            state_dim: Dimension of states
            action_dim: Dimension of actions
            hidden_dims: Dimensions of hidden layers in the validator network
            device: Device to use for computation
        """
        super().__init__(device)
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Network to classify validity
        layers = []
        input_dim = state_dim + action_dim
        
        for h_dim in hidden_dims:
            layers.extend([
                torch.nn.Linear(input_dim, h_dim),
                torch.nn.ReLU()
            ])
            input_dim = h_dim
            
        # Output layer for validity score
        layers.append(torch.nn.Linear(input_dim, 1))
        layers.append(torch.nn.Sigmoid())
        
        self.validity_network = torch.nn.Sequential(*layers).to(self.device)
        
        # Loss function for training
        self.loss_fn = torch.nn.BCELoss()
        
        # Optimizer
        self.optimizer = torch.optim.Adam(self.validity_network.parameters(), lr=1e-3)
    
    def train_step(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        labels: torch.Tensor
    ) -> float:
        """
        Perform one training step.
        
        Args:
            states: Batch of states, shape [batch_size, seq_len, state_dim]
            actions: Batch of actions, shape [batch_size, seq_len, action_dim]
            labels: Ground truth validity labels, shape [batch_size]
            
        Returns:
            Training loss
        """
        # Combine states and actions
        batch_size, seq_len, _ = states.shape
        flattened_states = states.view(batch_size, -1)
        flattened_actions = actions.view(batch_size, -1)
        
        combined = torch.cat([flattened_states, flattened_actions], dim=1)
        
        # Forward pass
        self.optimizer.zero_grad()
        validity_scores = self.validity_network(combined).squeeze(-1)
        
        # Compute loss
        loss = self.loss_fn(validity_scores, labels)
        
        # Backward pass
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def validate(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        task: Optional[Any] = None
    ) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """
        Validate plans using the learned validator.
        
        Args:
            states: Tensor of states in the trajectory
            actions: Tensor of actions in the trajectory
            task: Optional task specification
            
        Returns:
            Tuple of (validity mask, detailed validity information)
        """
        # Combine states and actions
        batch_size, seq_len, _ = states.shape
        flattened_states = states.view(batch_size, -1)
        flattened_actions = actions.view(batch_size, -1)
        
        combined = torch.cat([flattened_states, flattened_actions], dim=1)
        
        # Forward pass
        with torch.no_grad():
            validity_scores = self.validity_network(combined).squeeze(-1)
            
        # Apply threshold (default 0.5)
        validity = validity_scores >= 0.5
        
        # Detailed info
        validity_info = {
            "scores": validity_scores,
            "mean_score": validity_scores.mean().item(),
            "valid_rate": validity.float().mean().item()
        }
        
        return validity, validity_info
    
    def compute_violation_penalties(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        task: Optional[Any] = None
    ) -> torch.Tensor:
        """
        Compute violation penalties from the validity network.
        
        Args:
            states: Tensor of states in the trajectory
            actions: Tensor of actions in the trajectory
            task: Optional task specification
            
        Returns:
            Tensor of violation penalties for each trajectory
        """
        # Combine states and actions
        batch_size, seq_len, _ = states.shape
        flattened_states = states.view(batch_size, -1)
        flattened_actions = actions.view(batch_size, -1)
        
        combined = torch.cat([flattened_states, flattened_actions], dim=1)
        
        # Forward pass
        with torch.no_grad():
            validity_scores = self.validity_network(combined).squeeze(-1)
            
        # Convert to penalties (1 - validity)
        penalties = 1.0 - validity_scores
        
        return penalties


class HybridValidator(PlanValidator):
    """
    Hybrid validator combining rule-based and learning-based approaches.
    
    This validator leverages the strengths of both explicit rules and
    learned constraints for more robust validation.
    """
    
    def __init__(
        self,
        rule_validator: RuleBasedValidator,
        learned_validator: LearningBasedValidator,
        rule_weight: float = 0.7,
        learned_weight: float = 0.3,
        device: str = None
    ):
        """
        Initialize the hybrid validator.
        
        Args:
            rule_validator: Rule-based validator instance
            learned_validator: Learning-based validator instance
            rule_weight: Weight for rule-based validator
            learned_weight: Weight for learning-based validator
            device: Device to use for computation
        """
        super().__init__(device)
        self.rule_validator = rule_validator
        self.learned_validator = learned_validator
        self.rule_weight = rule_weight
        self.learned_weight = learned_weight
    
    def validate(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        task: Optional[Any] = None
    ) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """
        Validate plans using both validation approaches.
        
        Args:
            states: Tensor of states in the trajectory
            actions: Tensor of actions in the trajectory
            task: Optional task specification
            
        Returns:
            Tuple of (validity mask, detailed validation information)
        """
        # Get validation results from both validators
        rule_validity, rule_info = self.rule_validator.validate(states, actions, task)
        learned_validity, learned_info = self.learned_validator.validate(states, actions, task)
        
        # Combine results (require both to be valid by default)
        combined_validity = rule_validity & learned_validity
        
        # Detailed info
        combined_info = {
            "rule_validity": rule_info,
            "learned_validity": learned_info,
            "rule_valid_rate": rule_validity.float().mean().item(),
            "learned_valid_rate": learned_validity.float().mean().item(),
            "combined_valid_rate": combined_validity.float().mean().item()
        }
        
        return combined_validity, combined_info
    
    def compute_violation_penalties(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        task: Optional[Any] = None
    ) -> torch.Tensor:
        """
        Compute weighted violation penalties from both validators.
        
        Args:
            states: Tensor of states in the trajectory
            actions: Tensor of actions in the trajectory
            task: Optional task specification
            
        Returns:
            Tensor of violation penalties for each trajectory
        """
        # Get penalties from both validators
        rule_penalties = self.rule_validator.compute_violation_penalties(states, actions, task)
        learned_penalties = self.learned_validator.compute_violation_penalties(states, actions, task)
        
        # Combine with weights
        combined_penalties = (
            self.rule_weight * rule_penalties +
            self.learned_weight * learned_penalties
        )
        
        return combined_penalties


class TaskSpecificValidator(PlanValidator):
    """
    Task-specific validator that adapts to different tasks.
    
    This validator can customize its validation behavior based on a
    task embedding or specification, enabling flexible constraint checking.
    """
    
    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        task_dim: int,
        hidden_dims: List[int] = [128, 64, 32],
        device: str = None
    ):
        """
        Initialize the task-specific validator.
        
        Args:
            state_dim: Dimension of states
            action_dim: Dimension of actions
            task_dim: Dimension of task embeddings
            hidden_dims: Dimensions of hidden layers in the validator network
            device: Device to use for computation
        """
        super().__init__(device)
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.task_dim = task_dim
        
        # Network to classify validity
        layers = []
        input_dim = state_dim + action_dim + task_dim
        
        for h_dim in hidden_dims:
            layers.extend([
                torch.nn.Linear(input_dim, h_dim),
                torch.nn.ReLU()
            ])
            input_dim = h_dim
            
        # Output layer for validity score
        layers.append(torch.nn.Linear(input_dim, 1))
        layers.append(torch.nn.Sigmoid())
        
        self.validity_network = torch.nn.Sequential(*layers).to(self.device)
        
        # Loss function for training
        self.loss_fn = torch.nn.BCELoss()
        
        # Optimizer
        self.optimizer = torch.optim.Adam(self.validity_network.parameters(), lr=1e-3)
    
    def train_step(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        tasks: torch.Tensor,
        labels: torch.Tensor
    ) -> float:
        """
        Perform one training step.
        
        Args:
            states: Batch of states, shape [batch_size, seq_len, state_dim]
            actions: Batch of actions, shape [batch_size, seq_len, action_dim]
            tasks: Batch of task embeddings, shape [batch_size, task_dim]
            labels: Ground truth validity labels, shape [batch_size]
            
        Returns:
            Training loss
        """
        # Combine states, actions, and tasks
        batch_size, seq_len, _ = states.shape
        flattened_states = states.view(batch_size, -1)
        flattened_actions = actions.view(batch_size, -1)
        
        # Repeat tasks for each trajectory step if needed
        if tasks.dim() == 2:
            # Tasks are already [batch_size, task_dim]
            task_inputs = tasks
        else:
            raise ValueError("Tasks should have shape [batch_size, task_dim]")
        
        combined = torch.cat([flattened_states, flattened_actions, task_inputs], dim=1)
        
        # Forward pass
        self.optimizer.zero_grad()
        validity_scores = self.validity_network(combined).squeeze(-1)
        
        # Compute loss
        loss = self.loss_fn(validity_scores, labels)
        
        # Backward pass
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def validate(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        task: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """
        Validate plans using the task-specific validator.
        
        Args:
            states: Tensor of states in the trajectory
            actions: Tensor of actions in the trajectory
            task: Task embedding or specification
            
        Returns:
            Tuple of (validity mask, detailed validity information)
        """
        if task is None:
            raise ValueError("Task embedding is required for TaskSpecificValidator")
            
        # Combine states, actions, and task
        batch_size, seq_len, _ = states.shape
        flattened_states = states.view(batch_size, -1)
        flattened_actions = actions.view(batch_size, -1)
        
        # Ensure task has batch dimension
        if task.dim() == 1:
            task = task.unsqueeze(0).expand(batch_size, -1)
            
        combined = torch.cat([flattened_states, flattened_actions, task], dim=1)
        
        # Forward pass
        with torch.no_grad():
            validity_scores = self.validity_network(combined).squeeze(-1)
            
        # Apply threshold (default 0.5)
        validity = validity_scores >= 0.5
        
        # Detailed info
        validity_info = {
            "scores": validity_scores,
            "mean_score": validity_scores.mean().item(),
            "valid_rate": validity.float().mean().item()
        }
        
        return validity, validity_info
    
    def compute_violation_penalties(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        task: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Compute violation penalties, conditioned on the task.
        
        Args:
            states: Tensor of states in the trajectory
            actions: Tensor of actions in the trajectory
            task: Task embedding or specification
            
        Returns:
            Tensor of violation penalties for each trajectory
        """
        if task is None:
            raise ValueError("Task embedding is required for TaskSpecificValidator")
            
        # Combine states, actions, and task
        batch_size, seq_len, _ = states.shape
        flattened_states = states.view(batch_size, -1)
        flattened_actions = actions.view(batch_size, -1)
        
        # Ensure task has batch dimension
        if task.dim() == 1:
            task = task.unsqueeze(0).expand(batch_size, -1)
            
        combined = torch.cat([flattened_states, flattened_actions, task], dim=1)
        
        # Forward pass
        with torch.no_grad():
            validity_scores = self.validity_network(combined).squeeze(-1)
            
        # Convert to penalties (1 - validity)
        penalties = 1.0 - validity_scores
        
        return penalties