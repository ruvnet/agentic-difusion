class CodeGenerator:
    def __init__(self, tokenizer, syntax_model, diffusion_model):
        self.tokenizer = tokenizer
        self.syntax_model = syntax_model
        self.diffusion_model = diffusion_model

    def detect_language(self, specification):
        # Minimal stub for TDD: always return "python"
        return "python"

    def generate_code(self, specification, language=None, partial_code=None,
                     batch_size=4, precision="float32", device=None,
                     guidance_scale=1.5, temperature=0.7, use_rewards=True,
                     max_length=512, num_iterations=1):
        """
        Generate code from a specification, with performance options.
        
        Args:
            specification (str): Code specification
            language (str, optional): Programming language
            partial_code (str, optional): Partial code to complete
            batch_size (int, optional): Number of samples to generate
            precision (str, optional): Precision (e.g., "float32", "float16")
            device (str, optional): Device ("cpu", "cuda", etc.)
            guidance_scale (float, optional): Guidance scale for classifier-free guidance
            temperature (float, optional): Sampling temperature (lower = more deterministic)
            use_rewards (bool, optional): Whether to use reward models for quality assessment
            max_length (int, optional): Maximum length of generated code
            num_iterations (int, optional): Number of generation iterations for refinement
            
        Returns:
            str: Generated code (best sample based on rewards if batch_size > 1)
        """
        if not specification and not partial_code:
            raise ValueError("Either specification or partial_code must be provided")
            
        if language is None:
            language = self.detect_language(specification or "")
        
        # Prepare parameters for the diffusion model
        generation_kwargs = {
            "batch_size": batch_size,
            "precision": precision,
            "device": device,
            "guidance_scale": guidance_scale,
            "temperature": temperature,
            "use_rewards": use_rewards,
            "max_length": max_length,
        }
        
        # Remove None values for compatibility
        generation_kwargs = {k: v for k, v in generation_kwargs.items() if v is not None}
        
        best_code = None
        best_score = -1.0
        
        # Try multiple iterations if specified
        for iteration in range(num_iterations):
            if iteration > 0:
                print(f"Running iteration {iteration+1}/{num_iterations}")
            
            # Try both "generate" and "sample" methods for compatibility
            code = None
            
            # First try the "generate" method (preferred)
            if hasattr(self.diffusion_model, "generate"):
                try:
                    code = self.diffusion_model.generate(specification, language, partial_code, **generation_kwargs)
                except (TypeError, ValueError) as e:
                    print(f"Error in generate method: {e}")
                    # Fallback to simpler method call with fewer arguments
                    try:
                        code = self.diffusion_model.generate(specification, language, partial_code)
                    except Exception as e2:
                        print(f"Fallback generate also failed: {e2}")
            
            # If generate didn't work or isn't available, try "sample"
            if code is None and hasattr(self.diffusion_model, "sample"):
                try:
                    sample_result = self.diffusion_model.sample(
                        specification, language, partial_code, **generation_kwargs
                    )
                    
                    # Handle different return types
                    if isinstance(sample_result, list):
                        if sample_result:  # If list is not empty
                            code = sample_result[0]
                    else:
                        code = sample_result
                        
                except (TypeError, ValueError) as e:
                    print(f"Error in sample method: {e}")
                    # Fallback to simpler method call
                    try:
                        code = self.diffusion_model.sample(specification, language, partial_code)
                    except Exception as e2:
                        print(f"Fallback sample also failed: {e2}")
            
            # Validate the generated code
            if code is None:
                if iteration == num_iterations - 1:  # Last iteration
                    raise RuntimeError("Diffusion model failed to generate valid code")
                continue
            
            if not isinstance(code, str):
                code = str(code)
            
            # Evaluate the quality if rewards are enabled and we have multiple iterations
            if use_rewards and num_iterations > 1:
                quality_metrics = self.evaluate_quality(code, specification, language)
                current_score = quality_metrics.get("overall_score", 0.0)
                
                # Keep track of the best code based on quality score
                if current_score > best_score:
                    best_code = code
                    best_score = current_score
                    print(f"New best code with score: {best_score:.4f}")
            else:
                # If not using rewards or only one iteration, just use the current code
                best_code = code
        
        return best_code or "# Failed to generate valid code"

    def evaluate_quality(self, generated_code, specification=None, language="python"):
        """
        Evaluate the quality of generated code.
        
        Args:
            generated_code (str): The code to evaluate
            specification (str, optional): Original code specification
            language (str, optional): Programming language of the code
            
        Returns:
            dict: Quality metrics including syntax correctness, relevance, and overall reward
        """
        # Check if diffusion model has evaluate_code_quality method
        if hasattr(self.diffusion_model, 'evaluate_code_quality'):
            # Use the diffusion model's quality assessment
            return self.diffusion_model.evaluate_code_quality(
                generated_code, specification or "", language
            )
        
        # Fallback to using syntax model directly
        if hasattr(self.syntax_model, 'check_syntax'):
            syntax_correct = self.syntax_model.check_syntax(generated_code, language)
        else:
            # Default fallback
            syntax_correct = True
        
        # Import reward models directly if needed
        try:
            from agentic_diffusion.code_generation.rewards.syntax_reward import SyntaxReward
            from agentic_diffusion.code_generation.rewards.quality_reward import QualityReward
            from agentic_diffusion.code_generation.rewards.relevance_reward import RelevanceReward
            
            # Initialize reward models
            syntax_reward = SyntaxReward()
            quality_reward = QualityReward()
            relevance_reward = RelevanceReward()
            
            # Calculate rewards
            syntax_score = syntax_reward.evaluate(generated_code, language)
            quality_score = quality_reward.evaluate(generated_code, language)
            relevance_score = relevance_reward.evaluate(
                generated_code,
                reference=specification or "",
                language=language
            )
            
            # Calculate overall score
            weights = {"syntax": 0.4, "quality": 0.3, "relevance": 0.3}
            overall_score = (
                weights["syntax"] * syntax_score +
                weights["quality"] * quality_score +
                weights["relevance"] * relevance_score
            )
            
            return {
                "syntax_correct": syntax_correct,
                "syntax_score": syntax_score,
                "quality_score": quality_score,
                "relevance_score": relevance_score,
                "overall_score": overall_score
            }
        except ImportError:
            # Fallback if reward models can't be imported
            return {
                "syntax_correct": syntax_correct,
                "relevance_score": 0.8,
                "quality_score": 0.8,
                "overall_score": 0.8
            }